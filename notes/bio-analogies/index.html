<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

    <meta name="twitter:card" content="summary" />
    <meta name="twitter:creator" content="@ch402" />
    <meta property="og:url" content="https://colah.github.io/notes/bio-analogies/" />
    <meta property="og:title" content="Analogies between Biology and Deep Learning [rough note]" />
    <meta property="og:description"
        content="A list of advantages that make understanding artificial nerural networks much easier than biological ones." />

    <title>Analogies between Biology and Deep Learning [rough note] -- colah's blog</title>

    <link rel="stylesheet" href="../../fonts/Serif/cmun-serif.css" />
    <link rel="stylesheet" href="../../fonts/Serif-Slanted/cmun-serif-slanted.css" />

    <!--BOOTSTRAP-->
    <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <!--mobile first-->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!--removed html from url but still is html-->
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

    <!--font awesome-->
    <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

    <!--fonts: allan & cardo-->
    <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

    <link href="../../css/sticky-footer-navbar.css" rel="stylesheet">

    <link href="../../css/default.css" rel="stylesheet">

    <link href="../../comments/inlineDisqussions.css" rel="stylesheet">

    <!--Highlight-->
    <link href="../../highlight/styles/github.css" rel="stylesheet">

    <link href="../../favicon.ico" rel="shortcut icon" />

    <!--<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
    <script type="text/javascript"
        src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <style>
        .post {
            width: 170px;
            min-height: 175px;
            padding-left: 5px;
            padding-right: 5px;
            float: left;
            border-left: 1px solid #CCC;
            background-color: white;
        }

        div a:first-of-type .post {
            border-left: none;
        }

        .post:hover {
            filter: brightness(90%);
        }

        .post h3 {
            margin: 5px;
            font-size: 75%;
            text-align: center
        }

        .post h4 {
            margin: 0px;
            font-size: 50%;
            text-align: center
        }

        .post img {
            margin: 0px;
            padding: 2px;
            margin-bottom: 10px;
            width: 100%;
            height: 155px
        }
        ul li {
            margin-top: 12px;
        }
    </style>

    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-49811703-1', 'colah.github.io');
        ga('require', 'linkid', 'linkid.js');
        ga('require', 'displayfeatures');
        ga('send', 'pageview');

    </script>

</head>

<body>
    <div id="wrap">
        <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
            <div class="container">
                <!--Toggle header for mobile-->
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand active" href="../../" style="font-size:20px;">colah's blog</a>
                </div>
                <!--normal header-->
                <div class="navbar-collapse collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li><a href="../../"><span class="glyphicon glyphicon-pencil"></span> Blog</a></li>
                        <li><a href="../../about.html"><span class="glyphicon glyphicon-user"></span> About</a></li>
                        <li><a href="../../contact.html"><span class="glyphicon glyphicon-envelope"></span> Contact</a>
                        </li>
                    </ul>
                </div>
                <!--/.nav-collapse -->
            </div>
        </nav>


        <div id="content">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 main-article">
                        <h1>Analogies between Biology and Deep Learning [rough note]</h1>

                        <div class="info">
                            <p style="font-family:CMSS; font-size:120%">Posted on Oct 2, 2021</p>
                        </div>


                        <br>

                        <div
                            style='background: #F8F8F8; border: 1px solid #AAA; border-radius: 8px; padding: 12px; font-size: 90%; color: #444; font-style: italic;'>
                            <b> This article is a rough note.</b>
                            Writing rough notes allows me share more content, since polishing takes lots of time.
                            While I hope it's useful, it's likely lower quality and less carefully considered than my
                            usual articles.
                            It's very possible I wouldn't stand by this content if I thought about it more.
                        </div>

                        <br>
                        <br>

                        <style>
                            body {
                                text-align: left !important;
                            }

                            h2 {
                                margin-top: 70px !important;
                                font-size: 200% !important;
                            }

                            h1 div {
                                font-size: 80%;
                                color: #999;
                            }

                            h2 div {
                                margin-bottom: 8px;
                                font-size: 100%;
                                color: #999;
                                margin-top: 60px;
                            }

                            figcaption {
                                font-size: 80%;
                                color: #777;
                                line-height: 115%;
                            }

                            span.title-arrow {
                                margin-left: 12px;
                                margin-right: 12px;
                                font-size: 140%;
                            }

                            .main-article a {
                                color: #77C !important;
                            }
                        </style>


                        <p>
                            There are a number of exciting connections between physics and deep learning. Perhaps the
                            most discussed are scaling laws (e.g. <a href="https://arxiv.org/pdf/2001.08361.pdf">Kaplan
                                et
                                al, 2020</a>), but other connections are numerous (see e.g. <a
                                href="https://www.annualreviews.org/doi/full/10.1146/annurev-conmatphys-031119-050745">Bahri
                                et al, 2019</a> for a review).
                        </p>

                        <p> This essay is about a different set of analogies which I think are underrated:
                            analogies to biology. Where physics analogies often encourage us to zoom out and focus
                            on the big picture, analogies to biology often suggest looking more closely at
                            the details and internal structure of neural networks. </p>
                                <!-- -- which tend towards
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            connections to statistical physics ---->
                        <p> Below are a list of some analogies I find interesting. I've tried not to
                            spend too much time on the standard ones (eg. the neuroscience analogy and the evolution as
                            learning analogy), and instead focus on connections that may be less familiar.
                            These analogies will tend to be weaker and more exploratory than the physics connections
                            mentioned earlier, or the traditional biology analogies.
                            I find them interesting and generative to think about, but one wants to take them with a big grain of salt.</p>

                        <p>
                            Some caveats: (1) This post is biased towards work that I've been a part of, since
                            these
                            analogies have been very intertwined with thinking about my own work over the last few
                            years. (2) It's speculative and non-rigorous, as loose analogies often are. (3) It's a rough
                            note collecting random thoughts rather than highly-considered views.
                        </p>

                        <!--
                        <ul>
                            <li>Analogies
                                <ul>
                                    <li>Sub analogy</li>
                                    <li>Sub analogy</li>
                                    <li>Sub analogy</li>
                                    <li>Sub analogy</li>
                                    <li>Sub analogy</li>
                                </ul>
                            </li>
                            <li>Analogies
                                <ul>
                                    <li>Sub analogy</li>
                                    <li>Sub analogy</li>
                                    <li>Sub analogy</li>
                                    <li>Sub analogy</li>
                                </ul>
                            </li>
                        </ul>-->



                        <h2>General Analogies</h2>
                        
                        <h3>Symmetry / Segmentation <span class="title-arrow">↔</span> Weight-Tying</h3>
                        <p><i>Analogy: weights=DNA, weight-tying=segmentation, symmetry=symmetry</i></p>
                        
                        <p>In biology, <a href='https://en.wikipedia.org/wiki/Segmentation_(biology)'>segmentation</a> is when organisms have
                            bodies with repeated segments. <a href='https://en.wikipedia.org/wiki/Symmetry_in_biology'>Symmetry</a> is when
                            organisms have bodies with symmetries such as a reflection. A famous example is
                            <a href='https://en.wikipedia.org/wiki/Bilateria'>Bilateria</a> (of which humans are a part), which developed
                            bilateral symmetry early in the tree of life. Presumably segmentation and symmetry allow DNA to more efficiently
                            represent complex body plans (and thereby make evolution's exploration more efficient, see discussion of
                            evolvability later).
                        </p>
                        
                        <p>
                            In neural networks, weight-tying is when the same weights are used in multiple locations. This allows the network to
                            abstract repeated "functions" and use them multiple times. The most famous example is probably the <a
                                href="https://colah.github.io/posts/2014-07-Conv-Nets-Modular/">convolutional neural network</a>, which uses a
                            special version of weight tying to implement translation symmetry.
                            Weight tying allows neural networks to encode complex functions with many fewer weights, making them easier to
                            learn.
                        </p>
                        
                        <p>
                            Weight-tying looks, in some ways, quite similar to segmentation, and convolutional neural networks seem a little
                            similar to biological symmetry. But there's one major difference: complex weight-tying schemes, such as those in
                            convolutional neural networks, are typically designed by humans, while evolution learned segmentation and biological
                            symmetry.
                            Arguably, the Transformer architecture has recently moved neural networks closer to the learned symmetries of evolution.
                            (One could see the root of biological organisms exhibiting segmentation and symmetry as the existence of many cells with
                            the
                            same DNA, analogous to how weight-tied neural networks have many neurons with the same weights.
                            While architectures like convolutional networks force particular structures of interactions between neurons, the
                            transformer is much more flexible and instead learns those structures, making the analogy work better.)
                        
                        </p>

                        <!------------->
                        <h2>Interpretability Analogies</h2>
                        
                        <p>As biology studies organisms, interpretability studies artificial neural networks. As such, it is natural that many
                            analogies to biology will naturally connect to interpretability. In particular, many of the examples in this section
                            are
                            related to the <a href="https://distill.pub/2020/circuits/zoom-in/">circuits
                                perspective</a> on neural networks, a biology-flavored approach to interpretability that I'm involved
                            in.</p>

                        <h3>Neuroscience <span class="title-arrow">↔</span> Interpretability</h3>

                        <p><i>Analogy: model=brain</i></p>

                        <p>Artificial neural networks are historically inspired by neuroscience, but I
                            used to be pretty skeptical that the connection was anything more than
                            superficial. I've since come around: I now think this is a very deep
                            connection. The thing that personally persuaded me was that, in my own
                            investigations of what goes on inside neural networks, we kept finding things
                            that were previously discovered by neuroscientists. The most recent example of
                            this is <a href="https://distill.pub/2021/multimodal-neurons/">multimodal
                                neurons in CLIP</a>, which mirror a famous result in neuroscience. </p>

                        <p>If you think of artificial neural networks as analogous to biological neural networks, it's
                            also natural connect neuroscience (which studies biological networks) to interpretability
                            (which studies artificial ones). This is especially true of flavors of interpretability, like the <a
                                href="https://distill.pub/2020/circuits/zoom-in/">circuits</a> work I participate in,
                            which investigates individual neurons and their connections. In doing this work,
                            I feel like I've benefited from a lot of valuable lessons and gotten valuable feedback from the neuroscience community.
                            </p>

                        <p>But I also think it's worth keeping a very careful eye on ways in which neuroscience and
                            interpretability are very different. Interpretability has <a
                                href="http://colah.github.io/notes/interp-v-neuro/">many advantages</a> over
                            neuroscience, not the least of which is having access to all the weights. </p>

                        <h3>Anatomy <span class="title-arrow">↔</span> Interpretability</h3>

                        <p><i>Analogy: model=organism, weights=body?</i></p>

                        <p>Neural networks are almost like discovering a new, alien kind of organism.
                            Training neural networks samples alien evolution (subject to pressures defined
                            by the dataset). Interpretability is kind of like doing alien biology.</p>

                        <p>When we look inside neural networks, we're like early anatomists performing
                            the first dissections. We find all sorts of rich structure, both at high and
                            low-levels: </p>
                        <ul>
                            <li>"Tissues" - Neural networks have extremely
                                distinctive weight structures in later layers, which you might see as being
                                kind of analogous a distinct tissue in biology (<a href="https://distill.pub/2020/circuits/weight-banding/">Petrov et
                                    al, 2021</a>). Pushing the
                                analogy, one could almost imagine a "histology-style" approach to interpretability,
                                where neural networks are studied at the level of the "weight patterns" in different parts.
                            </li>
                            <li>"Brain Regions" - Neural networks have components that specialize in
                                particular tasks (eg. <a href="https://distill.pub/2020/circuits/branch-specialization/">Voss et al,
                                    2021</a>). This has a very natural analogy to regions of the brain and <a
                                    href="https://en.wikipedia.org/wiki/Neuroanatomy">neuroanatomy</a>. Stretching a bit
                                further, it might also be possible to see these structures as analogous to organs, in
                                that they're larger scale structures dedicated to a task.</li>
                            <li>Features / Circuits - This is the most abstract, but cracking
                                neural networks open and <a href='https://distill.pub/2020/circuits/early-vision/'>discovering features</a> and circuits
                                has the flavor of
                                discovering organic structures I imagine when I picture early anatomy. Since there are
                                so many features and circuits, it's perhaps natural to think of them as similar to
                                discovering tiny veins or other very small scale anatomical structure.
                                We don't yet have all the organizing principles or hypotheses, we're doing early
                                <a href="https://en.wikipedia.org/wiki/Descriptive_research">descriptive research</a>
                                like the early anatomists.
                                </li>
                        </ul>
                        
                        <p>Extensions of this analogy: If you look at multiple neural networks (or even look at multiple features within
                            a
                            network) you also start to get a flavor of taxonomy and comparative anatomy. If you look
                            over training, you get developmental anatomy.</p>



                        <h3>Motifs (Transcription Networks <span class="title-arrow">↔</span> Neural Networks)</h3>

                        <p><i>Analogy: model=transcription network</i></p>

                        <p>Transcription networks in genetics are graphs of excitation and inhibition
                            between genes. This is analogous to neural networks being graphs of excitation
                            and inhibition between neurons. The study of transcription networks makes extensive use of
                            recurring patterns called "circuit motifs" (<a
                                href="https://www.weizmann.ac.il/mcb/UriAlon/introduction-systems-biology-design-principles-biological-circuits">Alon,
                                2007</a>) to great effect. More generally, the
                            approach of studying graphs in terms of systems biology is a staple of systems biology.
                        </p>

                        <p>Circuit motifs can be found in the circuits of artificial neural networks. I think it's a
                            pretty powerful tool for simplifying the inner workings of neural networks.
                            A particularly powerful example is the <a
                                href="https://distill.pub/2020/circuits/equivariance/">equivariance motif</a> which
                            can simplify circuits in early vision by as much as 50x.</p>

                        <p>In some cases, the exact same motifs observed in transcription networks can be found in
                            convolutional neural networks if you unroll the motifs in time. For example, the <a
                                href="https://distill.pub/2020/circuits/zoom-in/#claim-2-dog">oriented dog head
                                circuit</a> can be seen as exhibiting an unrolled version of the "toggle switch" motif
                            (double-negative loop with positive autoregulation).</a></p>

                        <p>Unfortunately, many classic methods for studying motifs assume very sparse graphs, which
                            neural network weights are not by default. As a result, we can't trivially apply methods
                            from systems biology.</p>

                        <h3>Pleiotropy <span class="title-arrow">↔</span> Polysemanticity</h3>

                        <p><i>Analogy: neurons=genes</i></p>

                        <p><a href="https://en.wikipedia.org/wiki/Pleiotropy">Pleiotropy</a> is when a gene has multiple
                            unrelated effects. <a
                                href="https://distill.pub/2020/circuits/zoom-in/#claim-1-polysemantic">Polysemanticity</a>
                            is when a neuron does multiple
                            unrelated things. Possibly there's useful lessons to learn from one about the other.</p>

                        <!------------->
                        <h2>Evolution Analogies</h2>

                        <p>There's a well known connection between evolution and optimization (see <a
                                href="https://en.wikipedia.org/wiki/Evolutionary_algorithm"></a>evolutionary
                            algorithms</a>). In addition to this high-level connection, I think there are
                            many finer grained connections, specific to deep learning.</p>

                        <p>As a lay person with regards to biology, this section is heavily influenced by popular books
                            on evolution, especially the last few chapters of Dawkins' <i>The Ancestor's Tail</i>. It's
                            very possible I've misunderstood something</p>

                            <h3>Evolvability <span class="title-arrow">↔</span> Metalearning</h3>
                            
                            <p><i>Analogy: Model=organism, evolution=learning</i></p>
                            
                            <p><a href="https://en.wikipedia.org/wiki/Evolvability">Evolvability </a>is how effective a
                                species is at evolving. A number of major
                                evolutionary innovations seem to have been about increasing evolvability rather
                                than increasing zeroth-order fitness. For example, sexual reproduction switches evolution to a better optimization
                                algorithm (it can now optimize a gene pool of genes which combine in different ways). Or as another example,
                                segmentation and symmetry allowing DNA to more efficiently encode complex body plans (see earlier discussion).
                            </p>
                            
                            <p>Evolvability seems at least partially analogous to what we call "meta-learning" in machine learning, a broad
                                category of ideas around machine learning systems learning to learn better (see
                                discussion in <a href="https://arxiv.org/pdf/1907.06077.pdf">Gajewski
                                    et al, 2019</a>).
                                At the same time, if one tries to apply some of the ideas we include in meta-learning to evolution, they often seem
                                much broader than evolutionary biologists define evolvability...</p>
                            
                            
                            <h3>Fast Learning (Extension of Evolvability <span class="title-arrow">↔</span> Metalearning)</h3>
                            <p><i>Analogy: Model=organism, evolution=learning</i></p>
                            
                            
                            <p>
                                Even with the improvements we typically consider to be evolvability (discussed in the previous section), evolution is a
                                slow learner. Since it relies on
                                mutations, it can only happen on relatively large populations over several generations.
                            
                                Instead, evolution often develops mechanisms on top of evolution which allow for much faster adaptation.
                                We'll call these mechanisms "fast learning", since they don't seem to count as evolvability and I'm not
                                aware of a general term for this category of traits in biology.
                            </p>
                            <p>
                                The clearest example of biological fast learning is the nervous system in animals (a dedicated organ system for fast
                                behavioral adaptation!)
                                but there are many others.
                            
                                These fast learning
                                mechanisms often function at the level of individual organisms rather than populations and allow for change within a
                                lifetime.
                                (Often, these changes are not inherited by an organisms offspring, although there are cases where they are including
                                epigenetic inheritance, social learning, and antibodies in milk.)
                                Examples of biological fast learning include:
                            </p>
                            
                            <ul>
                                <li><b>Single-Cell Adaptive Behavior:</b> Certain single cellular organisms can exhibit adaptive learning, such as
                                    learning to avoid a stimuli associated with an electric shock. Somehow, this learning is implemented by chemical
                                    circuits within the cell, since they don't have a nervous system. See discussion in eg. <a
                                        href='https://royalsocietypublishing.org/doi/10.1098/rsif.2008.0344'>Fernando et al., 2008</a>.</li>
                                <li><b>Epigenetics:</b> Cells can bind chemicals to their DNA, modifying transcription and causing fast adaptation.
                                    For example, European holly trees develop more prickly leaves when herbivores graze on them, apparently through
                                    an
                                    epigenetic mechanism (<a href="https://academic.oup.com/botlinnean/article/171/3/441/2416188">Herrera and
                                        Bazaga, 2013</a>). In some cases, including the holly example, epigenetic changes may be heritable. </li>
                                <li><b>Adaptive Immune Systems:</b> The human immune system (and similar ones in other organisms) can learn to
                                    effectively respond to pathogens it has experienced before. (If disease immunity needed to be developed on
                                    evolutionary time scales, humans would be in a pretty terrible situation against rapidly evolving diseases!)
                                </li>
                                <li><b>The Nervous System / Intelligence:</b> The nervous system allows for animals to learn complex behavioral
                                    adaptations during their lifetimes. In some organisms, <a
                                        href="https://en.wikipedia.org/wiki/Social_learning_in_animals">social learning</a> allows organisms to
                                    transmit these behaviors to their offspring and others.</li>
                            </ul>
                            
                            <p>How does this relate to artificial neural networks? Some neural networks exhibit "fast learning"
                                which might be seen as very roughly analogous to these biological phenomena, in that it also allows for learning
                                on a much shorter timescale.</p>
                            
                            
                            <p>
                                After the initial success of deep learning at a variety of tasks in the mid 2010s, the research community began to
                                focus more on the amount of data needed, especially in the context of reinforcement learning.
                                Even when neural networks can learn to perform a task, they often need orders of magnitude more examples than a
                                human would.
                                Building on earlier work (eg. <a href="http://snowedin.net/tmp/Hochreiter2001.pdf">Hochreiter, 2001</a>),
                                researchers increasingly articulated metalearning as a response to this (eg. <a
                                    href="https://arxiv.org/pdf/1611.02779.pdf">Duan et al, 2017</a>; <a
                                    href="https://arxiv.org/pdf/1703.03400.pdf">Finn et al, 2017</a>; <a
                                    href="https://www.sciencedirect.com/science/article/pii/S1364661319300610">Botvinick, 2019</a>).
                                The idea is that we can create neural networks that "slowly learn" how to "fast learn" new tasks from a few
                                examples.
                                A particularly striking example of this is the in-context metalearning of GPT-3 (<a
                                    href="https://arxiv.org/pdf/2005.14165.pdf">Brown et al, 2020</a>): GPT-3 took an enormous amount of text to
                                train, but can often learn to perform new tasks from a handful of examples within it's context.
                            
                            </p>
                            
                            <p>
                                As metalearning developed, researchers have drawn analogies between slow learning and fast learning in machine learning
                                and evolution and intelligence in biology (eg.
                                <a href="http://joschu.net/docs/2018-09-aix.pdf">Schulman, 2018</a>; <a
                                    href="https://www.sciencedirect.com/science/article/pii/S1364661319300610">Botvinick, 2019</a>).
                                Just as intelligence allows organisms to learn within a single lifetime (in contrast to evolution taking generations),
                                machine learning's fast learning allows for learning within a single context or episode.
                                I think this is a really elegant and rich way to think about things, and can extended to other
                                examples of fast biological learning like the adaptive immune system.
                            </p>

                        <h3>Convergent Evolution <span class="title-arrow">↔</span> Feature Universality</h3>

                        <p><i>Analogy: model=organism, evolution=learning</i></p>

                        <p>Convergent evolution is often used to describe two similar seeming organisms
                            which aren't actually related, but simply evolved to be similar.
                            While entire organisms evolving to be similar is likely the most familiar example of convergent evolution, there's also
                            a more subtle version.
                            Often, two organisms will evolve to be similar in a particular way:
                            they'll both evolve to use the same chemical, or develop flying, or eyes.
                            Dolphins and bats are clearly very different organisms, but they independently evolved echolocation.</p>

                        <p>In neural networks, the same features and circuits form again and again across models (e.g.
                            <a href="https://arxiv.org/pdf/1511.07543.pdf">Li et al, 2016</a>; <a
                                href="https://distill.pub/2020/circuits/zoom-in/">Olah et al, 2020</a>; <a
                                href="https://distill.pub/2020/circuits/frequency-edges/">Voss et al, 2021a</a>; many
                            other results show something similar at an aggregate level e.g. <a
                                href="https://arxiv.org/pdf/1706.05806.pdf">Raghu et al, 2017</a>).
                            My collaborators and I often call this <a
                                href="https://distill.pub/2020/circuits/zoom-in/#claim-3">"universality"</a>.
                        </p>

                        <p>It seems natural to see these two phenomena as analogous. In both cases, an optimization process
                            (evolution or gradient descent) produces the same result independently, multiple times.
                            With that said, there are some caveats to this analogy Many popular
                            cases of convergent evolution are about convergence of capabilities (like flight, or
                            echolocation), but the universality of features is an internal property, perhaps more
                            analogous to convergence on the same chemical or metabolic innovations internally within an
                            organism. (Of course, convergence in capabilities also exists in neural networks, but isn't
                            very surprising.) Universality of circuits is even more specific: it's convergence on the
                            same
                            "code" to implement something internal, and is perhaps most analogous to the same mutation
                            arising multiple times independently.
                        </p>

                        <h3>Features and Circuits as the Unit of Selection</h3>
                        
                        <p>When we consider analogies between evolution and training neural networks, we tend to focus on the "evolution" of the
                            model has a whole. But models are composed of features, and it can be interesting to think about how those features
                            "evolve." I'm aware of two specific analogies:</p>
                        
                        <ul>
                            <li><b>Model=Organism, Features=Genes.</b> Thinking of genes (or genomic regions) <a
                                    href="https://en.wikipedia.org/wiki/Gene-centered_view_of_evolution">as the
                                    unit of selection</a>
                                rather than the whole organism
                                has been fruitful in evolution. We can think of features (or circuits) as the genes of
                                a network, being optimized by the training process to maximize their contribution to fitness (loss). For example, I
                                (low confidence) think that sometimes many neurons will try to detect similar
                                things early in training, then one does the best job and others shift to different tasks. This seems similar to
                                a
                                gene outcompeting variants in the gene pool.
                            </li>
                            <li><b>Model=Ecosystem, Features=Species.</b> We can also think of a model as an ecosystem under
                                evolution, with each feature as a species competing for a niche within that ecosystem.
                                For example, suppose there are two circuits which compute similar features. If one circuit
                                does a better job than the other, and the information they provide substantially overlaps,
                                one circuit will gradually be starved of positive gradient, until the neurons implementing
                                it can be captured for a different circuit.
                                This analogy becomes interesting when we start thinking about variations in the model
                                and training objective. For example, do larger models allow for more "biodiversity"
                                (variety of features with similar niches) or larger "megafauna" (larger and more complex circuits)?
                                How do different training objectives (climates?) affect the types of features that form (convergent evolution to
                                attractors of a climate)?</li>
                                </ul>

                        <p>There's a specific example which makes me particularly excited to think about this general type of
                            analogy (although perhaps the best specific analogy is different from the ones listed).
                            As one looks at progressively larger
                            language models, their aggregate performance varies smoothly, but specific capabilities
                            undergo discontinuous jumps (<a href="https://arxiv.org/pdf/2005.14165.pdf">Brown et al,
                                2020</a>). For example, basic arithmetic seems to undergo a discontinuous change at
                            about 10B parameters. It's tempting to think that this corresponds to a change in the
                            internal strategy the model uses for answering these questions, with a corresponding change
                            in features and circuits. If so, one could imagine two different strategies competing
                            internally within the model, and the less effective potentially being squeezed out.</p>


                        <!------------->
                        <h2>Things that seem like they should have an analogy but I'm not sure what it is</h2>

                        <h3>Feature Specialization</h3>

                        <p>Sometimes features seem to "split" as you study larger models. For example, every vision neural
                            network I've studied has <a href="https://distill.pub/2020/circuits/frequency-edges/">high-low frequency
                                detectors</a>, but some of the largest models
                            ones seem to split high-low frequency detectors into more fine-grained features. One model I looked at
                            (InceptionV3?) has both medium-low frequency detectors and high-medium frequency
                            detectors, instead of a generic high-low frequency detectors. Others have a variety of "texture contrast detectors."
                        </p>

                        <p>To the extent this is a general pattern relating model scale and features,
                            I think it's a very interesting one. And it feels very "biological"-y,
                            for lack of a better term! But I'm not sure what the right analogy is.
                            Here are a few candidates, none of which seem exactly right:</p>

                        <ul>
                            <li>In someways, it feels like a phylogenetic tree. You have the ancestor
                                species (generic feature) and descendant species which specialized into
                                different niches (the specialized features). But this analogy doesn't capture
                                the scaling aspect.</li>
                            <li>I've wondered if it could somehow correspond to
                                ecosystem size (see the model=ecosystem analogy above), with larger ecosystems (larger models) having more niches and
                                supporting more
                                specialized species (specialized features).
                                But I don't think this is generally true in the biological case.</li>
                            <li>Sometimes in evolution there are "genome duplication events" where the
                                entire genome is duplicated. Since there are two copies of each gene, one can
                                evolve in a different direction, free from being evolutionarily conserved.
                                As an analogy, this has several nice properties: we have a tree of features,
                                and a larger DNA sequence can have more children in that tree.
                                However, the fact that the analogy results from duplication events,
                                rather than being a purely emergent result of evolutionary optimization, makes the analogy
                                seem weaker.</li>
                        </ul>

                        <h2>Conclusion</h2>
                        
                        <p>
                            Most of these analogies are very speculative and exploratory, but I've found them interesting and generative to
                            think about, especially in the context of interpretability.
                        </p>

                        <!------------->
                        <h2>Acknowledgments</h2>
                        <p>Thanks to the extensive (and difficult to enumerate) list of people who have discussed these
                            ideas with me over the years. Thanks especially to Laura Gunsalus for patiently talking through these ideas with
                            me and answering genetics questions.</p>

                        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
                        <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

                    </div>
                    <div class="col-md-4"></div>
                </div>
            </div>
        </div>



    </div>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

    <script src="../../bootstrap/js/bootstrap.min.js"></script>

    <script src="../../highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script src="../../js/footnotes.js"></script>

    <script src="../../comments/inlineDisqussions.js"></script>

    <noscript>Enable JavaScript for footnotes, Disqus comments, and other cool stuff.</noscript>

</body>

</html>